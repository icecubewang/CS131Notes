# W3;L2

## HW2



**‚ö†Ô∏è Read the hint at the end!!! **‚ö†Ô∏è

 

This is an exercise in READING CODE üëÄ üëÄ üëÄ 

- Eggert says that we don't emphasize reading enough here at UCLA, only writing



You are going to be writing a program that looks at a grammar **recursively**, in many small parts, that will all assume that every other part of the program just ‚ú®  *works* ‚ú® 



Let's take a look at the hint code and see how it handles concatenation ("and")



`mom` = make OR matcher

`mam` = make a matcher

`mcm` = make CONCATENATION matcher

```ocaml
let rec mom p q =
	let pm = mam p (*pm = p matcher = make a matcher for p*)
	and qm = mam q in
	(*pm and qm follow the convention for HW2 of matcher (acceptor) (fragment) ((this is the opposite of the hint code))*)
	fun acc frag ->
	(* assume pm works, if not fall back on qm *)
		match pm acc frag with
		(*pm either fails and returns NONE or succeeds and returns the matching suffix *)
		| None -> qm acc frag
		(* if qm doesnt work just give up and go home *)
		| x -> x
		(* ^ pm succeeded so well that it returned the suffix, so just return x *)
```

Note that OR is the "easier" of the two (between `mom` and `mam`)

```ocaml
let rec mcm p q =
	let pm = mam p
	and qm = mam q in
	(* ^we start off the same way as last time *)
	(* now we have to glue together pm and qm in such a way that the resulting func will match the concatenation of q and p *)
	fun acc frag ->
		(* to do this, we pass Q into the acceptor for P, so Q can reject anything it doesnt like and then P can retry until maybe something passed and the two can communicate easily *)
		pm (fun psuff -> qm acc psuff) frag
```



Something that is common in functional programming is you will finish and have code with "eraser marks" where you can go back and optimize and clean some stuff up (even though the OG was correct too)

if you had `(fun x -> f x)`, you would say "why would u do that? just call f!"

Same thing with above

We can reduce `fun psuff -> qm acc psuff` to just `qm acc`

```ocaml
let rec mcm p q =
	let pm = mam p
	and qm = mam q in
	fun acc frag ->
		pm (qm acc) frag
```

Which is ALSO guilty of the exact same problem from above because:

```ocaml
fun acc frag ->
	pm (qm acc) frag
```

is just syntactic sugar üçØ for:

```ocaml
fun acc -> fun frag -> pm (qm acc) frag
```

and we can reduce that to just:

```ocaml
fun acc -> pm (qm acc)
```

and so we get:

```ocaml
let rec mcm p q =
	let pm = mam p
	and qm = mam q in
	fun acc ->
		pm (qm acc)
```

and we can *further* reduce that to this:

```ocaml
let rec mcm p q acc =
	(mam p) (mam qm acc)
```

**HOWEVER**, Eggert does üõë **NOT** üõë approve of this "optimization" because it actually makes the program LESS efficient - even though the code is shorter - because the program cannot pre-compile `pm` and `qm` but rather must build them at run-time instead. This will reduce efficiency by delaying compile-time operations until run-time. üèÉ



*Back to your regularly scheduled... programming*. ü•Å ü•Å üêç 

## Expressions

Three common properties relate to expressions:

1. Precedence
2. Associativity
3. Ambiguity

Although there are more, largely those that deal with *semantics* rather than syntax

4. Overloading
   1. What does `a+b` mean? Depends on the types of `a` and `b`
5. Mixed mode operations
   1. Overloading is one thing, you can have Int + Int and Float + Float or Double + Double, but what about...
   2. Int + Double
   3. The way some languages do this is by having more overloading (i.e. having a int + int operator, int + float operator, float + double operator) but this has scaling issues (every new type you have doubles the size of the required operators).
   4. Other languages use rules and conversions. 
6. Side effects üß™ in expressions
   1. This is not a problem we have encountered yet since we have completely abstained from side effects
   2. What about `(x=3) + (y=4)`
   3. Or, even worse, `(*p=3) + (*q=4)` WHERE `p==q`
7. Order of evaluations
   1. Not an issue in OCaml/functional programming in general üòé 
   2. But when you allow side effects in expressions, you run into this issue
      1. What about `return f(x) + g(y)` which you would think "Oh how nice and functional! This is perfect, order doesn't matter!" But what if `f` modifies some global variable that `g` reads? Then the order very much does matter.
   3. What is the order of evaluations in C++? 
      1. It is NOT left to right
      2. It COULD be left to right, right to left, random, whatever, the compiler is free to choose! üò± 
      3. Java ‚òïÔ∏è  is left to right
      4. Answer: `undefined` if order matters
   4. The sad truth is that if your program depends on order of evaluation, **your program is buggy** üêú and that's the üçµ 

## Compilers

Two main approaches:

### Software Tools Approach üëæ üõ† 

A "nice little pipeline" that divides the very large problem of running a program into several consecutive steps.

- GCC works this way
- Developed at Bell Labs - 1976

### IDE Approach üè° 

This still splits up the problem, but the way it does so differs. Coming from Smalltalk, it uses the object-oriented approach.

- Developed at Xerox üì† 
- Think of writing a compiler as building a large object that lives in RAM and cleverly walks through the code
- Backups were integral to this approach

### Hybrid

In the modern day, these two approaches are often combined, taking the best from both. After all, they both have the same goal: take source code üìñ  and translate it into machine code ü§ñ 

## Interpreters

What's the difference?

- Compilers - source code ‚û°Ô∏è **machine code**
  - The machine code runs on top of the bare hardware/machine
  - Complex translation

```
machine code
-----------
machine
```

- Interpreters - source code ‚û°Ô∏è **high-level representation**
  - Where the high-level representation (which can be just a decorated parse tree OR, more commonly, *byte code* üëæ and lives in RAM) and this representation is *executed* by the interpreter
  - Byte code - Intermediate representation designed for specific language
    - NOT the source code language
    - NOT machine code
  - Simple translation

```
byte codes
-----------
interpreter
-----------
machine
```

### PROS üëç 

Easier for debugging

Better compile time performance - if you want to quickly change your program and see those results instantly, you don't have to recompile again, just re-run

### CONS üëé 

No matter how efficient the interpreter, you must have the interpreter sit between the execution of the byte code and the machine and this means that the interpreter will have to execute several instructions for every one in the byte code, getting to the point which is that **interpreters are slower**

### Example

Say we have

```
long up;

switch (program[*p++]) {
    case ADD:
    	push(pop() + pop())
}
```

The interpreter does **NOT** translate into machine code

It translates those instructions itself and then executes them.

### What About A C Interpreter?

> Generally speaking, the "markets" for compiled languages and interpreted languages are different. Thus, when Eggert tried to sell people on a C interpreter, no one wanted to use it! This is because the "market" for C LIKE the fact that it is compiled. They want that extra performance.

## The Tale of Java ‚òïÔ∏è 

Java was originally interpreted. If you had `Foo.java`, it would be compiled into Java byte codes. These bytecodes are fed into the Java interpreter, which runs on top of the machine.

However, when Java started taking off, people wanted more performance. No matter how much they optimized the interpreter though, it just couldn't beat C, which is always compiled.

So why not just compile it?

Because a big selling point of Java - then and now - is that it can run on ANY machine running the Java interpreter and that you do NOT want your source code to be shipped. Machine code cannot do that.

So, what they did is embed a compiler *inside* the interpreter: **JIT (just-in time) compiler**.

This idea, started in Java, is now gaining in popularity, most notably with Java*Script*.

A JIT compiler can actually take advantage of things a regular compiler **cannot**.

- Suppose you have a function:

```
int foo (int x) {
    return bar(++x);
}
```

- A regular compiler cannot - even if `bar()` literally always just returns 0 - optimize out bar. You *must* go and look up `bar` in the lookup table and execute and all that.
  - Say that `foo` and `bar` are inside a class. What if you subclassed and overrode `bar`? It could be very different.
- A JIT compiler - however - can make **runtime observations** (namely that `bar` always returns 0) and make optimizations.
  - You can keep a runtime bytecode note of what happens in both the super- and sub-classes. This allows you to perform separate optimizations.

## Profiling

A key technique used in JIT compilers and even regular programs.

You record whatever performance metric you are concerned about üìù - say, CPU performance - and the profiler will keep track of this metric.

- You can do this both at the machine code and byte code levels and adjust execution accordingly.
- Not ALWAYS for **performance** though! Can actually be used to run test cases in more depth (making sure at the instruction level that the right things happened) to check **correctness.**

## Dynamic Linking ‚ö°Ô∏è 

Your program **modifies itself**

- Bad - but accurate - analogy: self-surgery! üòä 
- This can be a bit scary because once your program starts modifying itself, you lose control üò± 

## Names üë¶ üëß 

Seemingly a basic concept in programming languages/notations.

### Bindings üîó

*Binding* - A relationship between a name and a value.

```
int x = 27;
```

You would THINK this is fairly basic binding, namely that:
$$
x \rightarrow 27
$$
However, how, then, would you interpret this:

```
return sizeof x;
```

size of... 27???

Does 27 have a "size"?

Not really...

Therefore, `x` is more than just its value and is not so simple of a binding.