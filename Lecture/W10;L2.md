# W10;L2: The Last One

"A record number of topics on lecture"

## Cost Models

Before and during developing software, you should have a **cost model** that accounts for all monetary costs associated with the developmental process.

- Basically, focus on what is important so you can optimize the important stuff and ignore all the other üí© 

A *cost model* is an *approximate* mental model for how programs execute and the costs associated with its development and execution

- Often wrong due to the changing nature of computer science
- Largely intuitive
- Big-O notation talks about scalability of your algorithm ü§ñ 

Many costs!

- Time
  - Real vs CPU
- Memory
  - RAM vs cache vs secondary storage
- Power
  - Power vs energy
- Network access
  - Latency
  - Throughput

### Function Calls ‚òéÔ∏è 

**Assumed to be call by value in the following**

The caller is eagerly evaluating its arguments and now must make copies for the parameters (the specific convetions are machine dependent - on the x86, you copy the first 6 arguments to registers for example)

Now jump to callee!!

Callee must allocate a stack frame, save registers as needed, and execute the function body

The callee also copies its result to the return register (for C there is only one (`%rax`)) and must **de**allocate the frame

Now jump BACK to caller

#### A Complication

Since function calls are like super common, we want them to go as fast as possible.

So people have devised a few different systems

##### Inline Functions

The compiler merely substitutes the callee function's body into the caller

- Pretty common, C/++ have `inline` keyword for example
- Good for small functions where the bodies are small since most of the work for those functions is done in all the overhead (copying arguments and saving registers and allocating stack frames)

BUT** ‚ö†Ô∏è you can't use it all the time!

- Inlining BLOATS the code linearly for every function call
- NO recursion üîÆ 

##### Tail Call Optimization

If `f()` returns `g()` then you can save some steps by

#### Escape Analysis üö™

The compiler looks for üëÄ values that your function might "let out" üè° 

- ex:

```java
class C {
    Point location() { return new Point(x, y); }
    void showLocation() {
        Point p = location();
        system.out.println("@" + p.x + "," + p.y);
    }
}
```

- here `p` is only addressable to the `showLocation()` function (i.e. `p` is only used inside `showLocation` twice (once for x and once for y), never returning `p` or exposing it)
- so! we can do an optimization ‚ö°Ô∏è 
- we can put `p` only on the Java stack, **NOT** on the heap
  - i.e. no garbage collection! üóë üòä 

### Outdated Cost Models

- the keyword `register` in C
  - invented when compilers were dumb and couldn't do register allocation well
- `inline`
  - invented when compilers didn't inline automatically/based on the function

## Semantics üìñ

If you are trying to explain a program, you are concerned about two things:

- syntax üå≥ - the grammar and form of the language, p easy
- semantics üìñ - the meaning of the program, harder
  - CS 132

Semantics is traditionally split into two parts:

1. **static semantics** - the meaning you can gather without executing the program
   1. type checking
   2. scope checking
2. **dynamic semantics** - the meaning that depends on the running of the program
   1. harder üôÉ 
   2. **operational semantics** - supply an interpreter for your language which will tell you what operations are executed by a given program
      1. similar to a dictionary entry for the English word "the" - there is no concrete definition, just saying "the definite article"
      2. they're still useful tho!
   3. **axiomatic semantics** - i will explain what the program means by giving you the rules required to understand the program (axioms/rules of inference about the language)
      1. like writing a prolog interpreter
      2. except with actual mathematic logic, not code at all
   4. **denotational semantics** - explain a program by coming up with a function that maps inputs to outputs and will explain the program in the sense of "if you give the program this input it will generate this"
      1. you can construct the meaning of the program through *functional* programming

### Static Semantics

**Attribute Grammar** - every node in a parse tree also has *attributes* that tells you the static semantics of that symbol (terminal or non-terminal)

### Operational üîß Semantics for ML üê´ written in Prolog ü§ñ

```prolog
eval(
	expr,
	context,
	result
)

eval(
	(+)(E1 E2),
	context,
	result
)
// first evaluate E1 and get a value V1
// then evaluate E2 and get value V2
// V is V1 + V2
```

## History üìö 

**1957 - FORTRAN**

- arrays, loops, procedures
- made by IBM
- within a year, half of programmers were using FORTRAN
- people assumed it would be impossible, that it was artificial intelligence to have a program that compiles another program

**1959 - LISP**

- recursion
- S-expressions (programs and data have the same representation)

**Algol 60**

- anti-thesis of FORTRAN
- supported recursion
- biggest contribution? BNF
- IBM didn't have a spec for FORTRAN so you had to buy a compiler and run it

**Simula 67**

- introduced OOP

**1976 - C**

- descendant of Algol 68, PL1, etc.
- introduced system code written in high level languages (OS (Linux kernel))
- first language that let you successfully write system code

**C++**

- Simula + C
- enormous failure due to its complexity
- but obviously a big success

**Java**

- C++ and Smalltalk (descendant of Simula 67)

**Kotlin**

- heavily based on Java and Scala